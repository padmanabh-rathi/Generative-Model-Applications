{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP1rknd-1kZ8"
      },
      "source": [
        "Q2. Train and evaluate two separate Seq2Seq Transformer models (10 marks)\n",
        "1. English-to-French translation model\n",
        "2. French-to-English translation model\n",
        "3. Use english.txt and french.txt data from demo 3 folder\n",
        "\n",
        "Tasks:\n",
        "\n",
        "- Implement a Seq2Seq Transformer model for English-to-French translation\n",
        "- Train a second model with the same architecture but for French-to-English translation using the opposite dataset.\n",
        "- Insert novel sentences into the English-to-French model and collect the outputs.\n",
        "- Feed the translated French sentences into your French-to-English model.\n",
        "- Compare the outputs of the two models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup (imports, device, random seeds)"
      ],
      "metadata": {
        "id": "TkWudNkp2v_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the usual PyTorch and other libraries\n",
        "import math, os, re, random, io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# setting seed so as i want reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1reCjkO235C",
        "outputId": "1deee639-cce5-46e0-e0f9-667125136bcf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load english.txt & french.txt"
      ],
      "metadata": {
        "id": "dMoCgru2HKeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EN_PATH = \"english.txt\"\n",
        "FR_PATH = \"french.txt\"\n",
        "\n",
        "with io.open(EN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    en_lines = [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "with io.open(FR_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    fr_lines = [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "# I need the same number of aligned examples, so I'll cut to the shared min length.\n",
        "N = min(len(en_lines), len(fr_lines))\n",
        "en_lines = en_lines[:N]\n",
        "fr_lines = fr_lines[:N]\n",
        "\n",
        "print(\"Total aligned pairs:\", N)\n",
        "print(\"Example:\", en_lines[0], \"|||\", fr_lines[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEUpTLfz3DVL",
        "outputId": "53bb6a30-5e5c-4517-fdeb-832a1c50e534"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total aligned pairs: 154883\n",
            "Example: Go. ||| Va !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing + tokenization + vocabulary build"
      ],
      "metadata": {
        "id": "g96eL_IVHYbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing :\n",
        "# - lowercase\n",
        "# - punctuation as separate tokens\n",
        "# - split on whitespace\n",
        "\n",
        "def basic_clean(s):\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"([.,!?;:()\\\"“”'’\\-])\", r\" \\1 \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def tokenize(s):\n",
        "    return basic_clean(s).split()\n",
        "\n",
        "# Building vocabularies for both languages from the training text.\n",
        "from collections import Counter\n",
        "\n",
        "SPECIAL_TOKENS = {\n",
        "    \"PAD\": \"<pad>\",\n",
        "    \"SOS\": \"<sos>\",\n",
        "    \"EOS\": \"<eos>\",\n",
        "    \"UNK\": \"<unk>\",\n",
        "}\n",
        "PAD_ID, SOS_ID, EOS_ID, UNK_ID = 0, 1, 2, 3\n",
        "\n",
        "def build_vocab(lines, max_vocab_size=20000):\n",
        "    counter = Counter()\n",
        "    for ln in lines:\n",
        "        counter.update(tokenize(ln))\n",
        "    most_common = counter.most_common(max_vocab_size - 4)\n",
        "    itos = [SPECIAL_TOKENS[\"PAD\"], SPECIAL_TOKENS[\"SOS\"], SPECIAL_TOKENS[\"EOS\"], SPECIAL_TOKENS[\"UNK\"]] + [w for w, _ in most_common]\n",
        "    stoi = {w:i for i,w in enumerate(itos)}\n",
        "    return stoi, itos\n",
        "\n",
        "# Building English (src) and French (tgt) vocabs separately.\n",
        "SRC_MAX_VOCAB = 20000\n",
        "TGT_MAX_VOCAB = 20000\n",
        "\n",
        "src_stoi, src_itos = build_vocab(en_lines, SRC_MAX_VOCAB)\n",
        "tgt_stoi, tgt_itos = build_vocab(fr_lines, TGT_MAX_VOCAB)\n",
        "\n",
        "SRC_V = len(src_itos)\n",
        "TGT_V = len(tgt_itos)\n",
        "print(\"SRC vocab size:\", SRC_V, \"TGT vocab size:\", TGT_V)\n",
        "\n",
        "def encode_src(sentence):\n",
        "    toks = tokenize(sentence)\n",
        "    return torch.tensor([src_stoi.get(t, UNK_ID) for t in toks], dtype=torch.long)\n",
        "\n",
        "def encode_tgt(sentence):\n",
        "    toks = tokenize(sentence)\n",
        "    ids = [tgt_stoi.get(t, UNK_ID) for t in toks]\n",
        "    # For decoder, I want <sos> ... <eos>\n",
        "    return torch.tensor([SOS_ID] + ids + [EOS_ID], dtype=torch.long)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZGvCwiL3P2e",
        "outputId": "ebef4a4b-a816-4057-e67f-b755c81a9709"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC vocab size: 13728 TGT vocab size: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset + DataLoader"
      ],
      "metadata": {
        "id": "jdr9kGIaHfia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnFrDataset(Dataset):\n",
        "    def __init__(self, en_list, fr_list):\n",
        "        self.en_list = en_list\n",
        "        self.fr_list = fr_list\n",
        "    def __len__(self):\n",
        "        return len(self.en_list)\n",
        "    def __getitem__(self, idx):\n",
        "        src_ids = encode_src(self.en_list[idx])\n",
        "        tgt_ids = encode_tgt(self.fr_list[idx])\n",
        "        return src_ids, tgt_ids\n",
        "\n",
        "dataset = EnFrDataset(en_lines, fr_lines)\n",
        "\n",
        "# Simple train/val split\n",
        "val_ratio = 0.05\n",
        "val_size = int(len(dataset) * val_ratio)\n",
        "train_size = len(dataset) - val_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(SEED))\n",
        "print(\"Train:\", len(train_dataset), \"Val:\", len(val_dataset))\n",
        "\n",
        "# Collate: pad to the longest in batch and create masks for Transformer.\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
        "    return mask\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_seqs, tgt_seqs = zip(*batch)\n",
        "    src_lens = [len(s) for s in src_seqs]\n",
        "    tgt_lens = [len(t) for t in tgt_seqs]\n",
        "\n",
        "    src_pad = pad_sequence(src_seqs, batch_first=True, padding_value=PAD_ID)   # [B, S]\n",
        "    tgt_pad = pad_sequence(tgt_seqs, batch_first=True, padding_value=PAD_ID)   # [B, T]\n",
        "\n",
        "    tgt_input = tgt_pad[:, :-1]\n",
        "    tgt_output = tgt_pad[:, 1:]\n",
        "\n",
        "    src_key_padding_mask = (src_pad == PAD_ID)\n",
        "    tgt_key_padding_mask = (tgt_input == PAD_ID)\n",
        "\n",
        "    T = tgt_input.size(1)\n",
        "    tgt_mask = generate_square_subsequent_mask(T)\n",
        "\n",
        "    batch_dict = {\n",
        "        \"src\": src_pad,\n",
        "        \"tgt_in\": tgt_input,\n",
        "        \"tgt_out\": tgt_output,\n",
        "        \"src_key_padding_mask\": src_key_padding_mask,\n",
        "        \"tgt_key_padding_mask\": tgt_key_padding_mask,\n",
        "        \"tgt_mask\": tgt_mask\n",
        "    }\n",
        "    return batch_dict\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IayDioTB3lGQ",
        "outputId": "8d8c9bba-673b-4cee-e496-ccd8f6cfb0df"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 147139 Val: 7744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Seq2Seq model, English -> French"
      ],
      "metadata": {
        "id": "lc04SRgPHmh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Transformer layout with embeddings + sinusoidal positions.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)  # [max_len, 1, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        S = x.size(0)\n",
        "        return x + self.pe[:S]\n",
        "\n",
        "class TransformerSeq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model=256, nhead=8, num_layers=3, dim_ff=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.src_embed = nn.Embedding(src_vocab, d_model, padding_idx=PAD_ID)\n",
        "        self.tgt_embed = nn.Embedding(tgt_vocab, d_model, padding_idx=PAD_ID)\n",
        "        self.pos_enc   = PositionalEncoding(d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_ff,\n",
        "            dropout=dropout, batch_first=False\n",
        "        )\n",
        "        self.generator = nn.Linear(d_model, tgt_vocab)\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, tgt_in, src_key_padding_mask, tgt_key_padding_mask, tgt_mask):\n",
        "        src = src.transpose(0,1)\n",
        "        tgt_in = tgt_in.transpose(0,1)\n",
        "\n",
        "        src_emb = self.pos_enc(self.src_embed(src))\n",
        "        tgt_emb = self.pos_enc(self.tgt_embed(tgt_in))\n",
        "\n",
        "        memory = self.transformer.encoder(\n",
        "            src_emb,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "        out = self.transformer.decoder(\n",
        "            tgt_emb, memory,\n",
        "            tgt_mask=tgt_mask,  # [T,T]\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "        logits = self.generator(out)\n",
        "        return logits.transpose(0,1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def greedy_decode(self, src, max_len=60):\n",
        "\n",
        "        self.eval()\n",
        "        src_key_padding_mask = (src == PAD_ID)\n",
        "        src_t = src.transpose(0,1)\n",
        "        src_emb = self.pos_enc(self.src_embed(src_t))\n",
        "        memory = self.transformer.encoder(src_emb, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        ys = torch.tensor([[SOS_ID]], dtype=torch.long, device=src.device)\n",
        "        for _ in range(max_len-1):\n",
        "            tgt_emb = self.pos_enc(self.tgt_embed(ys.transpose(0,1)))\n",
        "            tgt_mask = generate_square_subsequent_mask(ys.size(1))\n",
        "            out = self.transformer.decoder(\n",
        "                tgt_emb, memory,\n",
        "                tgt_mask=tgt_mask,\n",
        "                tgt_key_padding_mask=None,\n",
        "                memory_key_padding_mask=src_key_padding_mask\n",
        "            )\n",
        "            logits = self.generator(out)\n",
        "            next_id = logits[-1, 0].argmax().unsqueeze(0).unsqueeze(0)\n",
        "            ys = torch.cat([ys, next_id], dim=1)\n",
        "            if next_id.item() == EOS_ID:\n",
        "                break\n",
        "        return ys.squeeze(0).tolist()\n"
      ],
      "metadata": {
        "id": "8adHFgwy33qq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train (10 epochs)"
      ],
      "metadata": {
        "id": "VXKnxUfyHwnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerSeq2Seq(SRC_V, TGT_V, d_model=256, nhead=8, num_layers=3, dim_ff=512, dropout=0.1).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "def run_epoch(data_loader, train=True):\n",
        "    model.train(train)\n",
        "    total_loss, total_tokens = 0.0, 0\n",
        "    for batch in data_loader:\n",
        "        src = batch[\"src\"].to(device)\n",
        "        tgt_in = batch[\"tgt_in\"].to(device)\n",
        "        tgt_out = batch[\"tgt_out\"].to(device)\n",
        "        src_kpm = batch[\"src_key_padding_mask\"].to(device)\n",
        "        tgt_kpm = batch[\"tgt_key_padding_mask\"].to(device)\n",
        "        tgt_mask = batch[\"tgt_mask\"].to(device)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        logits = model(src, tgt_in, src_kpm, tgt_kpm, tgt_mask)\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(logits.reshape(B*T, V), tgt_out.reshape(B*T))\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        tokens = (tgt_out != PAD_ID).sum().item()\n",
        "        total_loss += loss.item() * tokens\n",
        "        total_tokens += tokens\n",
        "\n",
        "    return total_loss / max(1,total_tokens)\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    train_loss = run_epoch(train_loader, train=True)\n",
        "    val_loss   = run_epoch(val_loader,   train=False)\n",
        "    print(f\"Epoch {ep:02d} | train xent/token: {train_loss:.4f} | val xent/token: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y1YAfVs4QGw",
        "outputId": "96d754f8-425e-42b7-87cf-a22dbe867d0b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train xent/token: 4.1678 | val xent/token: 3.0774\n",
            "Epoch 02 | train xent/token: 2.6643 | val xent/token: 2.1589\n",
            "Epoch 03 | train xent/token: 1.9062 | val xent/token: 1.5826\n",
            "Epoch 04 | train xent/token: 1.4709 | val xent/token: 1.3302\n",
            "Epoch 05 | train xent/token: 1.2171 | val xent/token: 1.2247\n",
            "Epoch 06 | train xent/token: 1.0502 | val xent/token: 1.0987\n",
            "Epoch 07 | train xent/token: 0.9325 | val xent/token: 1.0417\n",
            "Epoch 08 | train xent/token: 0.8431 | val xent/token: 1.0068\n",
            "Epoch 09 | train xent/token: 0.7748 | val xent/token: 0.9819\n",
            "Epoch 10 | train xent/token: 0.7187 | val xent/token: 0.9674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying out English sentences on the transformer"
      ],
      "metadata": {
        "id": "5GEqKf09H-TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "examples = [\n",
        "    \"I love to play soccer\",\n",
        "    \"I like cricket\",\n",
        "    \"I am from Mars\"\n",
        "]\n",
        "\n",
        "def ids_to_toks(ids, itos):\n",
        "    toks = []\n",
        "    for i in ids:\n",
        "        if i == SOS_ID:\n",
        "            continue\n",
        "        if i == EOS_ID:\n",
        "            break\n",
        "        toks.append(itos[i] if i < len(itos) else \"<unk>\")\n",
        "    return toks\n",
        "\n",
        "model.eval()\n",
        "for sample_en in examples:\n",
        "    src_ids = encode_src(sample_en).unsqueeze(0).to(device)\n",
        "    pred_ids = model.greedy_decode(src_ids, max_len=60)\n",
        "    pred_toks = ids_to_toks(pred_ids, tgt_itos)\n",
        "    print(\"EN:\", sample_en)\n",
        "    print(\"FR (greedy):\", \" \".join(pred_toks))\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beSJN6zR8Zxk",
        "outputId": "5992adcc-c3fe-466f-89f0-fccc3d81e59a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: I love to play soccer\n",
            "FR (greedy): j ' adore jouer au football .\n",
            "--------------------------------------------------\n",
            "EN: I like cricket\n",
            "FR (greedy): j ' aime le cricket .\n",
            "--------------------------------------------------\n",
            "EN: I am from Mars\n",
            "FR (greedy): je viens de mars .\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code blocks are for French to English"
      ],
      "metadata": {
        "id": "15vKGs8bBNjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuilding vocabs in opposite direction (FR=src, EN=tgt)\n",
        "import io, re, collections, torch, random\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "FR_PATH = \"french.txt\"\n",
        "EN_PATH = \"english.txt\"\n",
        "\n",
        "with io.open(FR_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    fr_lines = [ln.strip() for ln in f if ln.strip()]\n",
        "with io.open(EN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    en_lines = [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "N = min(len(fr_lines), len(en_lines))\n",
        "fr_lines, en_lines = fr_lines[:N], en_lines[:N]\n",
        "\n",
        "if \"basic_clean\" not in globals():\n",
        "    def basic_clean(s):\n",
        "        s = s.lower().strip()\n",
        "        s = re.sub(r\"([.,!?;:()\\\"“”'’\\-])\", r\" \\1 \", s)\n",
        "        s = re.sub(r\"\\s+\", \" \", s)\n",
        "        return s\n",
        "if \"tokenize\" not in globals():\n",
        "    def tokenize(s): return basic_clean(s).split()\n",
        "\n",
        "SPECIAL_TOKENS = {\"PAD\":\"<pad>\",\"SOS\":\"<sos>\",\"EOS\":\"<eos>\",\"UNK\":\"<unk>\"}\n",
        "PAD_ID, SOS_ID, EOS_ID, UNK_ID = 0, 1, 2, 3\n",
        "\n",
        "def build_vocab(lines, max_vocab_size=20000):\n",
        "    c = collections.Counter()\n",
        "    for ln in lines: c.update(tokenize(ln))\n",
        "    itos = [SPECIAL_TOKENS[\"PAD\"], SPECIAL_TOKENS[\"SOS\"], SPECIAL_TOKENS[\"EOS\"], SPECIAL_TOKENS[\"UNK\"]] \\\n",
        "           + [w for w,_ in c.most_common(max_vocab_size-4)]\n",
        "    stoi = {w:i for i,w in enumerate(itos)}\n",
        "    return stoi, itos\n",
        "\n",
        "src_stoi, src_itos = build_vocab(fr_lines, 20000)  # FR source vocab\n",
        "tgt_stoi, tgt_itos = build_vocab(en_lines, 20000)  # EN target vocab\n",
        "\n",
        "def encode_src_fr(s):\n",
        "    return torch.tensor([src_stoi.get(t, UNK_ID) for t in tokenize(s)], dtype=torch.long)\n",
        "\n",
        "def encode_tgt_en(s):\n",
        "    ids = [tgt_stoi.get(t, UNK_ID) for t in tokenize(s)]\n",
        "    return torch.tensor([SOS_ID] + ids + [EOS_ID], dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "aqlQxvXLBPm9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset/DataLoader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class FrEnDataset(Dataset):\n",
        "    def __init__(self, fr_list, en_list):\n",
        "        self.fr, self.en = fr_list, en_list\n",
        "    def __len__(self): return len(self.fr)\n",
        "    def __getitem__(self, i):\n",
        "        return encode_src_fr(self.fr[i]), encode_tgt_en(self.en[i])\n",
        "\n",
        "dataset = FrEnDataset(fr_lines, en_lines)\n",
        "val_size = int(0.05*len(dataset)); train_size = len(dataset)-val_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    return torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_seqs, tgt_seqs = zip(*batch)\n",
        "    src_pad = pad_sequence(src_seqs, batch_first=True, padding_value=PAD_ID)\n",
        "    tgt_pad = pad_sequence(tgt_seqs, batch_first=True, padding_value=PAD_ID)\n",
        "    tgt_in, tgt_out = tgt_pad[:, :-1], tgt_pad[:, 1:]\n",
        "    return {\n",
        "        \"src\": src_pad,\n",
        "        \"tgt_in\": tgt_in,\n",
        "        \"tgt_out\": tgt_out,\n",
        "        \"src_key_padding_mask\": (src_pad == PAD_ID),\n",
        "        \"tgt_key_padding_mask\": (tgt_in == PAD_ID),\n",
        "        \"tgt_mask\": generate_square_subsequent_mask(tgt_in.size(1))\n",
        "    }\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "vnmO1_acCEgt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train French -> English model (10 epochs)"
      ],
      "metadata": {
        "id": "ME_ogh8iIldh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FR→EN model + training loop\n",
        "\n",
        "model_fr2en = TransformerSeq2Seq(\n",
        "    src_vocab=len(src_itos),\n",
        "    tgt_vocab=len(tgt_itos),\n",
        "    d_model=256, nhead=8, num_layers=3, dim_ff=512, dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "optimizer = torch.optim.Adam(model_fr2en.parameters(), lr=3e-4)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model_fr2en.train(train)\n",
        "    total_loss, total_tok = 0.0, 0\n",
        "    for b in loader:\n",
        "        src = b[\"src\"].to(device)\n",
        "        tgt_in = b[\"tgt_in\"].to(device)\n",
        "        tgt_out = b[\"tgt_out\"].to(device)\n",
        "        src_kpm = b[\"src_key_padding_mask\"].to(device)\n",
        "        tgt_kpm = b[\"tgt_key_padding_mask\"].to(device)\n",
        "        tgt_mask = b[\"tgt_mask\"].to(device)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        logits = model_fr2en(src, tgt_in, src_kpm, tgt_kpm, tgt_mask)\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(logits.reshape(B*T, V), tgt_out.reshape(B*T))\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model_fr2en.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        ntok = (tgt_out != PAD_ID).sum().item()\n",
        "        total_loss += loss.item() * ntok\n",
        "        total_tok  += ntok\n",
        "\n",
        "    return total_loss / max(1, total_tok)\n",
        "\n",
        "# train for 10 epochs\n",
        "for ep in range(1, 10+1):\n",
        "    tr = run_epoch(train_loader, True)\n",
        "    va = run_epoch(val_loader,   False)\n",
        "    print(f\"[FR→EN] Epoch {ep:02d} | train xent/token {tr:.4f} | val {va:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O6x388fCMSs",
        "outputId": "56904295-d240-49d3-cc75-62c3e80ddc01"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FR→EN] Epoch 01 | train xent/token 3.8952 | val 2.9306\n",
            "[FR→EN] Epoch 02 | train xent/token 2.5262 | val 2.0142\n",
            "[FR→EN] Epoch 03 | train xent/token 1.7919 | val 1.4849\n",
            "[FR→EN] Epoch 04 | train xent/token 1.3642 | val 1.2248\n",
            "[FR→EN] Epoch 05 | train xent/token 1.1105 | val 1.0983\n",
            "[FR→EN] Epoch 06 | train xent/token 0.9478 | val 0.9972\n",
            "[FR→EN] Epoch 07 | train xent/token 0.8335 | val 0.9402\n",
            "[FR→EN] Epoch 08 | train xent/token 0.7453 | val 0.9093\n",
            "[FR→EN] Epoch 09 | train xent/token 0.6755 | val 0.8753\n",
            "[FR→EN] Epoch 10 | train xent/token 0.6197 | val 0.8574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed the given French line → print English output"
      ],
      "metadata": {
        "id": "oahbXwj2JEEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed French lines that were generated from English to French model and print English outputs\n",
        "\n",
        "def ids_to_toks(ids, itos):\n",
        "    toks = []\n",
        "    for i in ids:\n",
        "        if i == SOS_ID:\n",
        "            continue\n",
        "        if i == EOS_ID:\n",
        "            break\n",
        "        toks.append(itos[i] if i < len(itos) else \"<unk>\")\n",
        "    return toks\n",
        "\n",
        "examples_fr = [\n",
        "    \"j ' adore jouer au football .\",\n",
        "    \"j ' aime le cricket .\",\n",
        "    \"je viens de mars .\"\n",
        "]\n",
        "\n",
        "model_fr2en.eval()\n",
        "for sample_fr in examples_fr:\n",
        "    src_ids = encode_src_fr(sample_fr).unsqueeze(0).to(device)\n",
        "    pred_ids = model_fr2en.greedy_decode(src_ids, max_len=60)\n",
        "    pred_toks = ids_to_toks(pred_ids, tgt_itos)\n",
        "    print(\"FR:\", sample_fr)\n",
        "    print(\"EN:\", \" \".join(pred_toks))\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3pvjMgtGGrc",
        "outputId": "1928435b-9241-490f-d8a0-a7f36a6bdc1a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FR: j ' adore jouer au football .\n",
            "EN: i love to play football .\n",
            "--------------------------------------------------\n",
            "FR: j ' aime le cricket .\n",
            "EN: i like cricket .\n",
            "--------------------------------------------------\n",
            "FR: je viens de mars .\n",
            "EN: i ' m from march .\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the outputs of the two models."
      ],
      "metadata": {
        "id": "tPDypcrnJ6W4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When I compare the two models, I see that both are able to translate the sentences with similar meaning in both directions. The English-to-French model gave “j' adore jouer au football .” for “I love to play soccer,” while the French-to-English model returned “i love to play football .” for the same idea. The word “soccer” got mapped to “football,” which is still correct but shows some variation. For the sentence about cricket, both models translated almost perfectly without any difference. For the Mars example, the English-to-French model gave “je viens de mars .” which is correct, but the French-to-English model came back as “i ' m from march .” where “mars” was mistaken for the month “March.” This shows that the models capture the general meaning but can differ on specific word choices or named entities."
      ],
      "metadata": {
        "id": "0yeuu1G9J8p3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}