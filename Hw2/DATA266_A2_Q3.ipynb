{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "3. Explore following optimization techniques that can be performed while training and document your findings with a basic example code snippets: (5 points)\n",
        "\n",
        "- Tensor Creation (CPU vs GPU)\n",
        "- Weight Initialization\n",
        "- Activation Checkpointing\n",
        "- Gradient Accumulation\n",
        "- Mixed Precision Training"
      ],
      "metadata": {
        "id": "-lMvVMxi7G2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Tensor Creation (CPU vs GPU)"
      ],
      "metadata": {
        "id": "_Uroyncb7OXf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl9bHLYXZyWj",
        "outputId": "b4e897c7-5201-4593-c9be-816d2b2d43c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Name: Tesla T4\n",
            "\n",
            "[CPU] Matrix Multiplication Time: 2.6765 seconds\n",
            "[GPU] Matrix Multiplication Time: 0.0819 seconds\n"
          ]
        }
      ],
      "source": [
        "# Import Required Libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# The dataset has 8 features and 1 target column\n",
        "df = pd.read_csv('diabetes.csv', header=None)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.iloc[:, :-1].values   # Take all columns except the last one as features\n",
        "y = df.iloc[:, -1].values    # Take the last column as target variable\n",
        "\n",
        "# Set device for CPU\n",
        "device_cpu = torch.device('cpu')\n",
        "\n",
        "device_gpu = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Print which GPU is available and its name\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Using CPU only\")\n",
        "\n",
        "# Convert Data to PyTorch Tensors\n",
        "# Convert feature and label arrays to PyTorch tensors\n",
        "# `unsqueeze(1)` reshapes y to be a column vector\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# We will be using large square matrices to simulate heavy computation\n",
        "MATRIX_SIZE = 5000\n",
        "\n",
        "# Create two large random matrices on CPU\n",
        "a_cpu = torch.rand(MATRIX_SIZE, MATRIX_SIZE, device=device_cpu)\n",
        "b_cpu = torch.rand(MATRIX_SIZE, MATRIX_SIZE, device=device_cpu)\n",
        "\n",
        "# Start timing CPU matrix multiplication\n",
        "start_time = time.time()\n",
        "c_cpu = torch.matmul(a_cpu, b_cpu)  # Perform matrix multiplication\n",
        "cpu_time = time.time() - start_time\n",
        "print(f\"\\n[CPU] Matrix Multiplication Time: {cpu_time:.4f} seconds\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Create two large random matrices on GPU\n",
        "    a_gpu = torch.rand(MATRIX_SIZE, MATRIX_SIZE, device=device_gpu)\n",
        "    b_gpu = torch.rand(MATRIX_SIZE, MATRIX_SIZE, device=device_gpu)\n",
        "\n",
        "    _ = torch.matmul(a_gpu, b_gpu)\n",
        "\n",
        "    # Use torch.cuda.synchronize() to ensure accurate timing\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "    c_gpu = torch.matmul(a_gpu, b_gpu)\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start_time\n",
        "\n",
        "    print(f\"[GPU] Matrix Multiplication Time: {gpu_time:.4f} seconds\")\n",
        "else:\n",
        "    print(\"[GPU] Skipped: CUDA-compatible GPU not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obvervation :    \n",
        "\n",
        "On a 5000×5000 matrix multiply, the CPU took 2.6765 s and the Tesla T4 GPU took 0.0819 s, which is about a 32.7× speedup (2.6765 / 0.0819). I used torch.cuda.synchronize() before timing to avoid async timing errors on GPU. This shows that for large dense tensor ops, running the compute on the GPU is far faster; if CUDA isn’t available the code falls back to CPU."
      ],
      "metadata": {
        "id": "J9BeUKNf8YWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Weight Initialization"
      ],
      "metadata": {
        "id": "zx3FZEsm8fnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Fixing the seed makes initializations repeatable.\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# The file has no header; last column is the binary target.\n",
        "# Seperating X and Y features\n",
        "df = pd.read_csv('diabetes.csv', header=None)\n",
        "X = df.iloc[:, :-1].values  # all columns except last\n",
        "y = df.iloc[:, -1].values   # last column\n",
        "\n",
        "# Convert to tensors (float32); reshape y to [N, 1] for a single-output model.\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "\n",
        "input_dim = X_tensor.shape[1]  # should be 8\n",
        "hidden_dim1 = 32               # first hidden layer\n",
        "hidden_dim2 = 16               # second hidden layer\n",
        "output_dim = 1\n",
        "\n",
        "# Defining Model\n",
        "class DiabetesMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, h1, h2, out_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, h1)\n",
        "        self.fc2 = nn.Linear(h1, h2)\n",
        "        self.fc3 = nn.Linear(h2, out_dim)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "        # Initializations after creating layers.\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Layer 1 uses Xavier\n",
        "        # For layers followed by ReLU, use gain = sqrt(2).\n",
        "        nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "\n",
        "        # Layer 2 uses Kaiming (He) Normal\n",
        "        # He initialization is designed for ReLU non-linearities (good forward variance).\n",
        "        # Using mode='fan_in' preserves variance in the forward pass for ReLU.\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "        # Layer 3 uses Standard Normal (mean=0, std=0.02)\n",
        "        nn.init.normal_(self.fc3.weight, mean=0.0, std=0.02)\n",
        "        nn.init.zeros_(self.fc3.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act(self.fc2(x))\n",
        "        x = self.fc3(x)  # final linear output (logit)\n",
        "        return x\n",
        "\n",
        "# Instantiate the Model\n",
        "model = DiabetesMLP(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "\n",
        "# summarize initial weights\n",
        "# Printing mean/std to verify that different inits were applied.\n",
        "with torch.no_grad():\n",
        "    w1_mean, w1_std = model.fc1.weight.mean().item(), model.fc1.weight.std().item()\n",
        "    w2_mean, w2_std = model.fc2.weight.mean().item(), model.fc2.weight.std().item()\n",
        "    w3_mean, w3_std = model.fc3.weight.mean().item(), model.fc3.weight.std().item()\n",
        "\n",
        "print(\"fc1 (Xavier Uniform)  -> weight mean/std: {:.6f} / {:.6f}\".format(w1_mean, w1_std))\n",
        "print(\"fc2 (Kaiming Normal)  -> weight mean/std: {:.6f} / {:.6f}\".format(w2_mean, w2_std))\n",
        "print(\"fc3 (Normal 0,0.02)   -> weight mean/std: {:.6f} / {:.6f}\".format(w3_mean, w3_std))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY9R5fUGo8sM",
        "outputId": "646149e4-94ac-433a-ecae-2d651824940c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1 (Xavier Uniform)  -> weight mean/std: -0.008326 / 0.305070\n",
            "fc2 (Kaiming Normal)  -> weight mean/std: -0.002070 / 0.245711\n",
            "fc3 (Normal 0,0.02)   -> weight mean/std: 0.004340 / 0.018771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :    \n",
        "\n",
        "my findings are that for the 3-layer MLP I used different inits per layer: fc1 = Xavier-Uniform with ReLU gain, fc2 = Kaiming-Normal (ReLU), fc3 = Normal(0, 0.02). The weight stats show near-zero means and the expected stds: fc1 std 0.3051 vs the Xavier-ReLU expectation ≈ 0.316 for (fan_in=8, fan_out=32); fc2 std 0.2457 vs He expectation √(2/32) ≈ 0.25; fc3 std 0.0188 close to the target 0.02. No training was run as this is just the initialization check."
      ],
      "metadata": {
        "id": "0g6UwODZ8nxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3 Activation Checkpointing\n"
      ],
      "metadata": {
        "id": "DGW-5Iy09Wyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "# Checkpointed MLP Def\n",
        "class CheckpointedMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, h1, h2, out_dim, use_checkpoint=True):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, h1)\n",
        "        self.fc2 = nn.Linear(h1, h2)\n",
        "        self.fc3 = nn.Linear(h2, out_dim)\n",
        "        self.act = nn.ReLU()\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "        nn.init.normal_(self.fc3.weight, mean=0.0, std=0.02)\n",
        "        nn.init.zeros_(self.fc3.bias)\n",
        "\n",
        "    # Define small functional blocks so they can be checkpointed.\n",
        "    def _block1(self, x):\n",
        "        # Block 1 = ReLU( fc1(x) )\n",
        "        return self.act(self.fc1(x))\n",
        "\n",
        "    def _block2(self, x):\n",
        "        # Block 2 = ReLU( fc2(x) )\n",
        "        return self.act(self.fc2(x))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.training and self.use_checkpoint:\n",
        "            # Ensure checkpoint precondition: some input must require grad.\n",
        "            if not x.requires_grad:\n",
        "                x = x.detach()              # detach to avoid modifying upstream graph\n",
        "                x.requires_grad_(True)      # enable grad on input for checkpoint\n",
        "\n",
        "            # Wrap each hidden block with checkpoint to save activation memory.\n",
        "            x = checkpoint(self._block1, x)  # Checkpointed block 1\n",
        "            x = checkpoint(self._block2, x)  # Checkpointed block 2\n",
        "        else:\n",
        "            # Standard forward without checkpointing\n",
        "            x = self._block1(x)\n",
        "            x = self._block2(x)\n",
        "\n",
        "        # Final linear layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Instantiate model and training bits\n",
        "# Dimensions reused from previous question:\n",
        "model_ckpt = CheckpointedMLP(input_dim, hidden_dim1, hidden_dim2, output_dim, use_checkpoint=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_ckpt.to(device)\n",
        "\n",
        "# Tiny DataLoader to demonstrate backward with checkpointing.\n",
        "# Reuses X_tensor and y_tensor created earlier.\n",
        "ds = TensorDataset(X_tensor, y_tensor)\n",
        "loader = DataLoader(ds, batch_size=128, shuffle=True)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_ckpt.parameters(), lr=1e-3)\n",
        "\n",
        "model_ckpt.train()  # keep model in training mode so checkpointing is active\n",
        "for step, (xb, yb) in enumerate(loader):\n",
        "    xb = xb.to(device)  # move batch to device (CPU/GPU)\n",
        "    yb = yb.to(device)  # move targets to device\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)  # reset grads for this step\n",
        "    logits = model_ckpt(xb)                # forward pass (hidden blocks are checkpointed)\n",
        "    loss = criterion(logits, yb)           # BCEWithLogitsLoss on raw logits\n",
        "    loss.backward()                        # backward pass (recomputes checkpointed activations)\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "qq_Pni9YG50z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :     \n",
        "\n",
        "I wrapped the two hidden ReLU blocks (fc1→ReLU and fc2→ReLU) with torch.utils.checkpoint only during training. I also set x.requires_grad_(True) to satisfy the checkpoint requirement. With this, the model saves activation memory in the forward pass and recomputes those activations in backward, so peak memory goes down but each step takes a bit longer. In eval mode (or if I set use_checkpoint=False) it runs normally without checkpointing. I kept the same inits from 3.2 (Xavier for fc1, He for fc2, small normal for fc3). The loop doesn’t print anything—it's just a regular training pass with checkpointing turned on."
      ],
      "metadata": {
        "id": "m-CnAoDo91pE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4 Gradient Accumulation"
      ],
      "metadata": {
        "id": "RzxoKNxd97Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the model in training mode\n",
        "model_ckpt.train()\n",
        "\n",
        "# Clear any existing gradients before starting accumulation\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "# Tracking how many micro-batches have contributed grads in the current window\n",
        "_accum_counter = 0\n",
        "\n",
        "# Counts how many times we've actually updated parameters (i.e., effective steps)\n",
        "effective_step = 0\n",
        "\n",
        "# Iterate over micro-batches from the DataLoader\n",
        "for _step, (xb, yb) in enumerate(loader, start=1):\n",
        "    # Move current micro-batch to the active device\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    # Forward pass on the micro-batch\n",
        "    logits = model_ckpt(xb)\n",
        "\n",
        "    # Compute the loss for current micro-batch.\n",
        "    # loss.backward() repeatedly and sum grads across 'accumulation_steps' micro-batches,\n",
        "    # the resulting gradient magnitude matches using one large batch in a single step.\n",
        "    loss = criterion(logits, yb) / accumulation_steps\n",
        "\n",
        "    # Backpropagate the SCALED loss\n",
        "    loss.backward()\n",
        "\n",
        "    # One more micro-batch has contributed to the gradient sum\n",
        "    _accum_counter += 1\n",
        "\n",
        "    # When we've seen 'accumulation_steps' micro-batches, apply one optimizer step\n",
        "    if _accum_counter == accumulation_steps:\n",
        "        # Update parameters ONCE using the accumulated gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # Clear gradients so the next accumulation window starts fresh\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Book-keeping: we performed one \"effective\" update\n",
        "        effective_step += 1\n",
        "\n",
        "        # 'loss' here is the SCALED value from the last micro-batch in the window.\n",
        "        print(f\"effective_step={effective_step} | loss={loss.item():.6f}\")\n",
        "\n",
        "        # Reset the counter for the next accumulation window\n",
        "        _accum_counter = 0\n",
        "\n",
        "if _accum_counter > 0:\n",
        "    # Apply one final step using whatever has accumulated\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    effective_step += 1\n",
        "    # Print Statement indicating we applied a final partial update\n",
        "    print(f\"effective_step={effective_step} | (final partial window)\")\n"
      ],
      "metadata": {
        "id": "H_Oh4FN1HIM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1e61b2-21b7-4851-f1d2-8e49c1d9e8a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "effective_step=1 | loss=0.171629\n",
            "effective_step=2 | (final partial window)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :     \n",
        "\n",
        "With gradient accumulation, I combined multiple micro-batches before updating the weights, so I got one optimizer step per accumulation window instead of per batch. The output shows exactly that: one full window produced effective_step=1 | loss=0.171629 (this loss is the scaled micro-batch loss from the last batch in that window), and then a second line effective_step=2 | (final partial window), which means the dataset size wasn’t divisible by accumulation_steps and we applied one final update with a partial window. This confirms accumulation is working: fewer optimizer steps, stable per-step loss reporting, and no increase to the micro-batch size (so peak memory stays the same)"
      ],
      "metadata": {
        "id": "wpYKTFHw-Ajj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.5 Mixed Precision Training"
      ],
      "metadata": {
        "id": "Ao7TljkX_Ay1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "try:\n",
        "    device\n",
        "except NameError:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "try:\n",
        "    accumulation_steps  # just to check existence\n",
        "except NameError:\n",
        "    accumulation_steps = 1  # default to no accumulation if not set earlier\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    # CUDA path will use float16 autocast + real GradScaler\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    def amp_autocast():\n",
        "        # autocast returns a context manager\n",
        "        return torch.cuda.amp.autocast(dtype=torch.float16)\n",
        "else:\n",
        "    # CPU path defines a no-op-ish scaler and try CPU autocast with bfloat16\n",
        "    class _NoOpScaler:\n",
        "        def scale(self, x): return x\n",
        "        def step(self, opt): opt.step()\n",
        "        def update(self): pass\n",
        "        def unscale_(self, opt): pass\n",
        "    scaler = _NoOpScaler()\n",
        "\n",
        "    def amp_autocast():\n",
        "        # Using CPU autocast if available\n",
        "        try:\n",
        "            return torch.autocast(device_type='cpu', dtype=torch.bfloat16)\n",
        "        except AttributeError:\n",
        "            from contextlib import nullcontext\n",
        "            return nullcontext()\n",
        "\n",
        "model_ckpt.to(device)\n",
        "model_ckpt.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "# AMP training loop with one print per effective step\n",
        "_accum_counter = 0\n",
        "effective_step = 0\n",
        "\n",
        "for _step, (xb, yb) in enumerate(loader, start=1):\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    # Forward + loss under mixed precision autocast\n",
        "    with amp_autocast():\n",
        "        logits = model_ckpt(xb)             # raw logits\n",
        "        loss = criterion(logits, yb)        # BCEWithLogitsLoss on logits\n",
        "        loss = loss / accumulation_steps    # scale for gradient accumulation\n",
        "\n",
        "    # Backward with GradScaler\n",
        "    scaler.scale(loss).backward()\n",
        "    _accum_counter += 1\n",
        "\n",
        "    if _accum_counter == accumulation_steps:\n",
        "\n",
        "        # torch.nn.utils.clip_grad_norm_(model_ckpt.parameters(), max_norm=1.0)\n",
        "\n",
        "        scaler.step(optimizer)      # applies update\n",
        "        scaler.update()             # adjusts scale on CUDA; no-op on CPU\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        effective_step += 1\n",
        "        print(f\"effective_step={effective_step} | loss={loss.item():.6f}\")\n",
        "\n",
        "        _accum_counter = 0\n",
        "\n",
        "# Handle a final partial accumulation window\n",
        "if _accum_counter > 0:\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    effective_step += 1\n",
        "    print(f\"effective_step={effective_step} | (final partial window)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYlUtwTO5eAq",
        "outputId": "d231b467-a9d0-4685-b56e-2bc7533db074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2003020927.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/tmp/ipython-input-2003020927.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast(dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "effective_step=1 | loss=0.170615\n",
            "effective_step=2 | (final partial window)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obvervation :    \n",
        "\n",
        "For mixed precision, I ran the training loop with AMP (autocast + GradScaler) and gradient accumulation. The console showed two things: (1) FutureWarning messages saying the torch.cuda.amp API is deprecated—so I should switch to torch.amp.GradScaler('cuda') and torch.amp.autocast('cuda') going forward—and (2) the step prints: effective_step=1 | loss=0.170615 followed by effective_step=2 | (final partial window). That means one full accumulation window produced an update with a scaled loss of ~0.1706, and then the dataloader ended with a partial window that triggered a final update. No NaNs or errors showed up, so the scaler handled precision fine."
      ],
      "metadata": {
        "id": "2x3DRnPJAh4W"
      }
    }
  ]
}