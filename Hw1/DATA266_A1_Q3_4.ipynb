{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Implement matrix multiplication using CUDA C programming. (5 points)\n",
        "- Use blocks and threads for parallel computation, clearly explain the blocks and threads in the code.\n",
        "- Profile the CUDA program using nvprof and nsys(Modern NVIDIA GPUs (CUDA 11 and newer))"
      ],
      "metadata": {
        "id": "rb4ymWbUczBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaGVqJBHdhBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2c6114-6814-4f1b-b3b2-506829467dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.12/dist-packages (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGfblNfZ8OzQ",
        "outputId": "62da2cd8-31f5-461c-8b2f-a5026120bbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nvcc4jupyter extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc4jupyter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile my_cuda_code.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// Defining the dimension of square matrices we will use in our code\n",
        "#define N 512  // Matrix size: N x N\n",
        "\n",
        "\n",
        "// CUDA Kernel for Matrix Multiplication\n",
        "// Each GPU thread computes one element of the output matrix C\n",
        "\n",
        "__global__ void matrixMulCUDA(float *A, float *B, float *C, int width) {\n",
        "    // Calculate the row index of the element this thread will compute\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    // Calculate the column index of the element this thread will compute\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Check if the thread is within matrix bounds\n",
        "    if (row < width && col < width) {\n",
        "        float sum = 0.0f;\n",
        "\n",
        "        // Compute dot product of the corresponding row of A and column of B\n",
        "        for (int k = 0; k < width; k++) {\n",
        "            sum += A[row * width + k] * B[k * width + col];\n",
        "        }\n",
        "\n",
        "        // Store the computed value in the output matrix C\n",
        "        C[row * width + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float);  // Total size in bytes for each matrix\n",
        "\n",
        "\n",
        "    // Allocate host memory (CPU)\n",
        "\n",
        "    float *h_A = (float *)malloc(size);  // Host matrix A\n",
        "    float *h_B = (float *)malloc(size);  // Host matrix B\n",
        "    float *h_C = (float *)malloc(size);  // Host result matrix C\n",
        "\n",
        "    // Initializing host matrices with sample values\n",
        "    // A filled with 1.0, B filled with 2.0 for simplicity and chekcing the output\n",
        "\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        h_A[i] = 1.0f;\n",
        "        h_B[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "\n",
        "    // Allocate device memory (GPU)\n",
        "\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc((void **)&d_A, size);  // Device matrix A\n",
        "    cudaMalloc((void **)&d_B, size);  // Device matrix B\n",
        "    cudaMalloc((void **)&d_C, size);  // Device result matrix C\n",
        "\n",
        "    // Copy input data from host (CPU) to device (GPU)\n",
        "\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "    // Defining thread and block configuration\n",
        "    // Each block contains 16x16 threads = 256 threads per block\n",
        "\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "\n",
        "    // Grid size calculated to cover full N x N matrix\n",
        "    // Each block handles 16 rows × 16 cols, so we need N/16 blocks per dimension\n",
        "    // (N + 15)/16 ensures we round up for any leftover rows/cols\n",
        "\n",
        "    dim3 blocksPerGrid((N + 15) / 16, (N + 15) / 16);\n",
        "\n",
        "    // Launch the kernel on the GPU\n",
        "\n",
        "    matrixMulCUDA<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Wait for kernel to complete before copying result back\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Check for any kernel launch errors\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Copy the result matrix from device back to host\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    // Output\n",
        "    // Print the first element as a basic check\n",
        "    // Expected value: 512 * (1.0 * 2.0) = 1024.0\n",
        "\n",
        "    printf(\"C[0] = %f\\n\", h_C[0]);\n",
        "\n",
        "    // Free allocated memory\n",
        "\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "    free(h_A); free(h_B); free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AfqaSRi9FTo",
        "outputId": "b903b1af-5e50-4cfe-cf08-0dfaae19f047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting my_cuda_code.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above CUDA matrix multiplication program, we use a 2D grid of 2D thread blocks to parallelize the computation. Each block has 16×16 = 256 threads, and the grid is sized so that all elements of a 512×512 matrix are covered. This results in a 32×32 grid of blocks, giving us a total of 1024 blocks and 262,144 threads—one for each element in the output matrix. Inside the kernel, each thread figures out its row and column position in the matrix using the formulas row = blockIdx.y * blockDim.y + threadIdx.y and col = blockIdx.x * blockDim.x + threadIdx.x. This tells each thread exactly which element of the matrix it's responsible for computing. Each thread then computes the value for one element in the result matrix by taking the dot product of a row from matrix A and a column from matrix B."
      ],
      "metadata": {
        "id": "reWuUadbkgd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o my_cuda_code my_cuda_code.cu\n"
      ],
      "metadata": {
        "id": "Le6OvPE7MAy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./my_cuda_code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPTSVdIwAqC_",
        "outputId": "4261c8c1-1ff1-452c-a4b2-a0b2d545cea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C[0] = 1024.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./my_cuda_code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBoJv7YWU2Wl",
        "outputId": "ed86dbc5-8c79-4194-821c-9c1d85aa1e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==27551== NVPROF is profiling process 27551, command: ./my_cuda_code\n",
            "C[0] = 1024.000000\n",
            "==27551== Profiling application: ./my_cuda_code\n",
            "==27551== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   80.64%  1.1472ms         1  1.1472ms  1.1472ms  1.1472ms  matrixMulCUDA(float*, float*, float*, int)\n",
            "                   12.29%  174.91us         2  87.455us  87.391us  87.519us  [CUDA memcpy HtoD]\n",
            "                    7.07%  100.51us         1  100.51us  100.51us  100.51us  [CUDA memcpy DtoH]\n",
            "      API calls:   98.30%  176.42ms         3  58.807ms  3.0820us  176.35ms  cudaMalloc\n",
            "                    0.77%  1.3737ms         3  457.89us  239.71us  821.57us  cudaMemcpy\n",
            "                    0.64%  1.1516ms         1  1.1516ms  1.1516ms  1.1516ms  cudaDeviceSynchronize\n",
            "                    0.13%  230.64us         3  76.880us  14.890us  114.43us  cudaFree\n",
            "                    0.08%  137.23us         1  137.23us  137.23us  137.23us  cudaLaunchKernel\n",
            "                    0.07%  131.67us       114  1.1540us     106ns  53.031us  cuDeviceGetAttribute\n",
            "                    0.01%  10.951us         1  10.951us  10.951us  10.951us  cuDeviceGetName\n",
            "                    0.00%  5.0020us         1  5.0020us  5.0020us  5.0020us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.4250us         3     475ns     125ns  1.0320us  cuDeviceGetCount\n",
            "                    0.00%     803ns         2     401ns     130ns     673ns  cuDeviceGet\n",
            "                    0.00%     682ns         1     682ns     682ns     682ns  cuDeviceTotalMem\n",
            "                    0.00%     621ns         1     621ns     621ns     621ns  cuModuleGetLoadingMode\n",
            "                    0.00%     295ns         1     295ns     295ns     295ns  cudaGetLastError\n",
            "                    0.00%     205ns         1     205ns     205ns     205ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./my_cuda_code\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvqQNX_UU9KL",
        "outputId": "188d5106-96e9-4342-9e0a-c2ee117d10dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nsys: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Implement matrix addition (C = A + B) for 1024×1024 matrices using CUDA C with 2D thread blocks (16×16 threads per block). In your code comments, explain how 2D thread indexing works and why it's more intuitive for matrix operations than 1D configuration. (5 points)"
      ],
      "metadata": {
        "id": "qNqnCp7AZ6nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_add.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 1024  // Define the size of the square matrix: 1024 x 1024\n",
        "\n",
        "// CUDA Kernel for Matrix Addition\n",
        "\n",
        "__global__ void matrixAdd(float *A, float *B, float *C, int width) {\n",
        "\n",
        "    // 2D THREAD INDEXING EXPLANATION\n",
        "\n",
        "    // Each thread is assigned to compute one unique element in the output matrix C.\n",
        "    // To find the position of the element this thread is responsible for,\n",
        "    // we compute its row and column based on:\n",
        "    //\n",
        "    // - blockIdx: index of the block within the grid (x = columns, y = rows)\n",
        "    // - blockDim: number of threads per block (e.g., 16x16 = 256 threads)\n",
        "    // - threadIdx: index of the thread within its own block\n",
        "    //\n",
        "    // This results in the global row and column index of the thread:\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y; // Row index for the thread\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x; // Column index for the thread\n",
        "\n",
        "    // Convert the (row, col) 2D position into a 1D index for linear memory access\n",
        "    int index = row * width + col;\n",
        "\n",
        "    // Check if the thread is within matrix bounds (for edge cases)\n",
        "    if (row < width && col < width) {\n",
        "        // Each thread adds corresponding elements from A and B\n",
        "        C[index] = A[index] + B[index];\n",
        "    }\n",
        "\n",
        "    // 2D thread indexing is more intuitive for matrix operations because:\n",
        "    // - It directly maps threads to matrix elements using (row, col) coordinates\n",
        "    // - This matches how we naturally access matrices in C: C[row][col]\n",
        "    // - It avoids the need to manually convert from 1D thread IDs using\n",
        "    //   division (%) and multiplication, making the code easier to read and maintain\n",
        "    // - It's especially helpful for large matrices where visualizing and debugging\n",
        "    //   becomes more complex with 1D indexing\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float);  // Total size of each matrix in bytes\n",
        "\n",
        "    // =======================\n",
        "    // Host Memory Allocation\n",
        "    // =======================\n",
        "    float *h_A = (float *)malloc(size);  // Matrix A on host\n",
        "    float *h_B = (float *)malloc(size);  // Matrix B on host\n",
        "    float *h_C = (float *)malloc(size);  // Result matrix C on host\n",
        "\n",
        "    // Initialize host matrices\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        h_A[i] = 1.0f;  // Fill matrix A with 1.0\n",
        "        h_B[i] = 2.0f;  // Fill matrix B with 2.0\n",
        "    }\n",
        "\n",
        "    // =======================\n",
        "    // Device Memory Allocation\n",
        "    // =======================\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc((void **)&d_A, size);  // Allocate matrix A on device\n",
        "    cudaMalloc((void **)&d_B, size);  // Allocate matrix B on device\n",
        "    cudaMalloc((void **)&d_C, size);  // Allocate result matrix C on device\n",
        "\n",
        "    // Copy input matrices from host to device\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // =======================\n",
        "    // Thread & Block Configuration\n",
        "    // =======================\n",
        "    // Each block has 16 x 16 = 256 threads (2D configuration)\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "\n",
        "    // Grid size is calculated to ensure full matrix coverage\n",
        "    // (N + 15) / 16 ensures we round up to cover all rows/columns\n",
        "    dim3 blocksPerGrid((N + 15) / 16, (N + 15) / 16);\n",
        "\n",
        "    // Launch the CUDA kernel\n",
        "    matrixAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Wait for the kernel to complete\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy the result matrix from device back to host\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print a sample output to verify if the code is giving ecpected output of 3\n",
        "    printf(\"C[0] = %f \", h_C[0]);\n",
        "\n",
        "    // Free all allocated memory\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "    free(h_A); free(h_B); free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh9zFpEAZ-FF",
        "outputId": "6703d220-39ef-44cd-c3e1-c63f37705d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o matrix_add matrix_add.cu\n"
      ],
      "metadata": {
        "id": "sxf1JO6CbtQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_add\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_HbuTipb86n",
        "outputId": "6c1c343f-28a9-4154-af28-82da8479b233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C[0] = 3.000000 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./matrix_add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMu72xJpdzxq",
        "outputId": "d7b4664b-1c26-4f84-b590-a191b22f5a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==27676== NVPROF is profiling process 27676, command: ./matrix_add\n",
            "C[0] = 3.000000 ==27676== Profiling application: ./matrix_add\n",
            "==27676== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   50.71%  1.6457ms         1  1.6457ms  1.6457ms  1.6457ms  [CUDA memcpy DtoH]\n",
            "                   47.54%  1.5427ms         2  771.37us  765.01us  777.72us  [CUDA memcpy HtoD]\n",
            "                    1.75%  56.671us         1  56.671us  56.671us  56.671us  matrixAdd(float*, float*, float*, int)\n",
            "      API calls:   96.92%  179.30ms         3  59.767ms  70.527us  179.16ms  cudaMalloc\n",
            "                    2.62%  4.8450ms         3  1.6150ms  937.61us  2.9229ms  cudaMemcpy\n",
            "                    0.27%  506.67us         3  168.89us  108.28us  199.28us  cudaFree\n",
            "                    0.07%  133.38us         1  133.38us  133.38us  133.38us  cudaLaunchKernel\n",
            "                    0.07%  132.52us       114  1.1620us     105ns  54.285us  cuDeviceGetAttribute\n",
            "                    0.03%  59.414us         1  59.414us  59.414us  59.414us  cudaDeviceSynchronize\n",
            "                    0.01%  9.8390us         1  9.8390us  9.8390us  9.8390us  cuDeviceGetName\n",
            "                    0.00%  4.9370us         1  4.9370us  4.9370us  4.9370us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.2650us         2  1.1320us     117ns  2.1480us  cuDeviceGet\n",
            "                    0.00%  1.4390us         3     479ns     123ns  1.1120us  cuDeviceGetCount\n",
            "                    0.00%     418ns         1     418ns     418ns     418ns  cuModuleGetLoadingMode\n",
            "                    0.00%     392ns         1     392ns     392ns     392ns  cuDeviceTotalMem\n",
            "                    0.00%     263ns         1     263ns     263ns     263ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./matrix_add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jQlmQPQeBZk",
        "outputId": "ccab1303-5516-46eb-9b0a-39030990b5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nsys: command not found\n"
          ]
        }
      ]
    }
  ]
}